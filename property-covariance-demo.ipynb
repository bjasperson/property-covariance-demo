{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring covariance between properties across scales\n",
    "**Authors**: Benjamin Jasperson, Harley T. Johnson\n",
    "\n",
    "**GitHub URL**: https://github.com/bjasperson/property-covariance-demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background Information\n",
    "\n",
    "This workbook explores the use of interatomic potential (IP)-generated data to uncover correlations between fundamental microscopic properties, which we call canonical properties, and large-scale quantities of interest (QoI).\n",
    "It closely follows the work outlined in [this manuscript](https://doi.org/10.1016/j.actamat.2025.120722).\n",
    "\n",
    "As a proof-of-principle, we'll use grain boundary (GB) energy as our QoI. \n",
    "Specifically, we'll look at how canonical properties relate to the scaling factor in the universal lattice matching (LM) model of Runnels et al. (2016).\n",
    "You don't need to worry about how this coefficient is defined; see the paper for details. Just think of it as a scalar measure of grain boundary energy.\n",
    "\n",
    "By the end of this workbook, you will have:\n",
    "1. Explored a dataset of IP-generated property results, uncovering correlations that can be used in a multiscale regression model.\n",
    "2. Developed a regression model using canonical properties identified through k-fold cross-validation.\n",
    "3. Used the regression model with first-principles indicator properties to make a prediction for the GB scaling coefficient from first-principles canonical properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "\n",
    "from itertools import combinations\n",
    "from textwrap import wrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data\n",
    "To begin, we need to import our project data. The data we will use is originally from [this manuscript](https://arxiv.org/abs/2411.16770), [published here](https://doi.org/10.1016/j.actamat.2025.120722). You can find the original paper repo [here](https://github.com/Johnson-Research-Group/gb_covariance).\n",
    "\n",
    "The data includes IP-generated canonical properties, along with an LM model scaling factor calculated from individual symmetric-tilt GB energy simulations.\n",
    "\n",
    "**Task**: Using pandas, download the CSV file with this [link](https://github.com/bjasperson/property-covariance-demo/blob/main/data/gb_data.csv?raw=true), save the dataframe as `df_data`, and look at the first five rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_link = \"https://github.com/bjasperson/property-covariance-demo/blob/main/data/gb_data.csv?raw=true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data review\n",
    "It is important to get a feel for the data that you are working with. To make plotting a little easier, we have a csv file with key/value pairs to convert the variable names into human readable format.  \n",
    "\n",
    "**Task**: create a function that will import the label csv file, located at `./data/label_dict.csv`, and convert it to a dictionary called `label_dict` using pandas, for use with plotting. Return the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_label_dict(label_dict_path = \"https://github.com/bjasperson/property-covariance-demo/blob/main/data/label_dict.csv?raw=true\"):\n",
    "    \"\"\"import the ./data/label_dict.csv file, convert it to a dictionary\n",
    "\n",
    "    Args:\n",
    "        label_dict_path (str, optional): path to csv file with key/value pairs.\n",
    "\n",
    "    Returns:\n",
    "        dict: a dictionary with df column names as keys, and human-readable label as value\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "    return label_dict\n",
    "\n",
    "label_dict = import_label_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's write a function that calculates the [correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) between two variables. \n",
    "\n",
    "**Task**: given our dataframe, create a function that uses [Pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html) to calculate the correlation coefficient between each set of variables. First, make a copy of the dataframe and replace the column names with the human readable version from the label dictionary using list comprehension. Next, apply the `corr` method to the copied dataframe to create `df_corr`. Finally, sort both the columns *and* rows of `df_corr` based on how correlated the variable is to the scaling factor, `$E_0$`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_df(df, label_dict):\n",
    "    \"\"\"create df with correlation values\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): our `df_data` property dataframe \n",
    "        label_dict (dict): the label dict from above, to convert label keys to human readable format\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: correlation coefficient between canonical properties and coeff ($E_0$). Sort both the columns and rows by how correlated they are with the coeff $E_0$.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    return df_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to look at how the different fundamental properties relate with each other and our Quantity of Interest (GB energy coefficient). To aid in this, we will look at two plots. First, let's plot a heat map of the correlation coefficient. \n",
    "\n",
    "**Task**: first, save the `df_data` columns to a list and remove the non-property columns (\"crystal_type\", \"species\", \"model\"). Write a function that calls the `correlation_df` function you previously created to get the correlation dataframe. Finally, use the Seaborn `heatmap` function to plot the correlation coefficient dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_plot(df, \n",
    "              label_list, \n",
    "              label_dict, \n",
    "              annot = False, \n",
    "              figsize = (10,10),\n",
    "              annotation_fontsize = 6,\n",
    "              tick_fontsize = 6,\n",
    "              ):\n",
    "    \"\"\"create a heatmap of the correlation coefficient results\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): our `df_data` property dataframe\n",
    "        label_list (list): list of property names (str) to include\n",
    "        label_dict (dict): the label dict from above, to convert label keys to human readable format\n",
    "        annot (bool, optional): adds the correlation coefficient value to each square. Defaults to False.\n",
    "        figsize (tuple, optional): set the figure size for plt.subplots. Defaults to (10,10).\n",
    "        annotation_fontsize (int, optional): annotation fontsize. Defaults to 6.\n",
    "        tick_fontsize (int, optional): tick fontsize. Defaults to 6.\n",
    "    \"\"\"\n",
    "    df_corr = correlation_df(df[label_list], label_dict)\n",
    "\n",
    "\n",
    "    return\n",
    "\n",
    "label_list = df_data.columns.to_list()\n",
    "label_list.remove(\"crystal_type\")\n",
    "label_list.remove(\"species\")\n",
    "label_list.remove(\"model\")\n",
    "corr_plot(df_data, label_list, label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how properties that are highly correlated with each other appear as square blocks in the plot. This makes it easy to identify groups of properties that can easily be substituted for each other other in the model. \n",
    "\n",
    "Next, let's look at pairplots of the most correlated features. \n",
    "\n",
    "**Task**: create a function to create pairplots of a set of properties from the property dataframe. Use Seaborn pairplot, and set the marker color (`hue`) based on `species`. Explore different combinations of pairplots for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairplot_fun(df, \n",
    "                 params_list,  \n",
    "                 label_dict, \n",
    "                 height=1.5,\n",
    "                 ):\n",
    "    \"\"\"create pairplot for select indicator properties\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): dataframe of data that was previously imported\n",
    "        params_list (list): list of parameter strings to use\n",
    "        label_dict (dict): the label dict from above, to convert label keys to human readable format\n",
    "        height (float): pairplot height. Defaults to 2.0.\n",
    "        xlims (list): list of limits to use for plotting\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    return\n",
    "\n",
    "# a set of parameters to get you started\n",
    "plot_param_list = [\"coeff\", \"lattice_constant_fcc\", \"lattice_constant_bcc\", 'intr_stack_fault_energy_fcc', 'unstable_stack_energy_fcc']\n",
    "pairplot_fun(df_data, plot_param_list, label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some questions to ponder:\n",
    "1. Did you find any factors that you think are worth including?\n",
    "2. Do you see any outliers that we need to remove? How will you decide what is an outlier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying indicator properties\n",
    "\n",
    "Now that we have our data, we want to figure out which canonical properties will be best for our model. To do that, we'll use a [linear regression model](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares) with [k-fold cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html#k-fold)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop regression model\n",
    "\n",
    "**Task**: Write a function that returns a sklearn linear regression pipeline. Your pipeline should include the following steps:\n",
    "- StandardScaler: for scaling the input values; helpful for SVR, used in LR for consistency. \n",
    "- KNNImputer: for filling in the missing property values in our data. Set `n_neighbors=2` and `keep_empty_features=True`\n",
    "- LinearRegression: the linear regression model (`linear_model.LinearRegression()`).\n",
    "\n",
    "Combine these into a `Pipeline` to facilitate easy use ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regr_pipeline():\n",
    "    \"\"\"creates a linear regression pipeline for modeling purposes\n",
    "\n",
    "    Returns:\n",
    "        sklearn.pipeline.Pipeline: linear regression pipeline\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    return pipe\n",
    "\n",
    "pipe = linear_regr_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to make a list of the properties we are interested in considering as indicator properties for our model. A starting point is provided below.\n",
    "\n",
    "**If time**: explore different properties, and see if any new additions result in improved models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of parameters to explore\n",
    "params_list = ['lattice_constant_fcc',\n",
    "                'bulk_modulus_fcc', 'c11_fcc', 'c12_fcc', 'c44_fcc',\n",
    "                'extr_stack_fault_energy_fcc', 'intr_stack_fault_energy_fcc', 'unstable_stack_energy_fcc'\n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the parameter list, we want to make a list of possible combinations up to a certain number of factors. \n",
    "\n",
    "**Task**: write a function that uses the `combinations` function from `itertools` to make a list of lists with possible combinations of factors up to `n_factor_max`. Return the list for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subsets_list(factor_list, n_factor_max):\n",
    "    \"\"\"create all possible combinations of factors from a list, up to n_factor_max\n",
    "\n",
    "    Args:\n",
    "        factor_list (list): a list of individual factors (str) to consider\n",
    "        n_factor_max (int): maximum number of factors to consider in one model\n",
    "\n",
    "    Returns:\n",
    "        list: a list of lists, with each sub-list containing the factors (str) to include (e.g., ['c11_fcc', 'c12_fcc'])\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    return subsets\n",
    "\n",
    "subsets = create_subsets_list(params_list, n_factor_max = 3)\n",
    "print(subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our list of possible factor combinations to consider. If we fit our model to the entire dataset, we are not going to get a realistic picture of the test error. So, we will use the `cross_val_score` package from scikit-learn to perform cross validation. Read more about it [here](https://scikit-learn.org/stable/modules/cross_validation.html).\n",
    "\n",
    "**Task**: write a function that iterates over each subset of the possible factor combinations. For each subset, use the `cross_val_score` package to perform cross-validation and return a score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factor_select_cv(df, pipe, subsets, label=\"coeff\", cv=None, scoring='r2'):\n",
    "    \"\"\"evaluates combinations of factors\n",
    "    \n",
    "    return a dataframe that includes one row per factor combination. list of parameters w/ cv score\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): dataframe of data that was previously imported\n",
    "        pipe (sklearn.pipeline.Pipeline): scikit-learn pipeline\n",
    "        subsets (list): a list of lists, with each sub-list containing the factors (str) to include (e.g., ['c11_fcc', 'c12_fcc'])\n",
    "        label (str, optional): the QoI label to use. Defaults to \"coeff\".\n",
    "        cv (int, optional): int, cross-validation generator or an iterable, Determines the cross-validation splitting strategy. Defaults to None.\n",
    "        scoring (str, optional): A str (see model evaluation documentation) or a scorer callable object / function with signature scorer(estimator, X, y) which should return only a single value. Defaults to 'r2'.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: a dataframe of CV results\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run repeated k-fold CV and get the results. \n",
    "\n",
    "**Task**: Call your previously defined `factor_select_cv` function with `RepeatedKFold` for the cv-generator (`n_splits=10`, `n_repeats=5`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"kfold_models_lr\"\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state = 12345)\n",
    "df_results = factor_select_cv(df_data,\n",
    "                              pipe, \n",
    "                              subsets, \n",
    "                              cv=cv, \n",
    "                              scoring='neg_root_mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Sort the values based on \"cv_score\", with `ascending=False`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.sort_values(\"cv_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick the top performing model and make a regression model to use. Make predictions and compare predicted versus actual. We will use `KFold` to divide our data up into k folds, so we don't overestimate our model performance.\n",
    "\n",
    "**Task**: write a function that will fit a pipe against our IP data, using the best combination of parameters. Use `KFold` to split our data 5 ways. For each fold, fit a new model to the training data and make a prediction for the test data using the pipe. When done, add the predictions to our `df_data` dataframe as \"coeff_pred\" and return the updated df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ip_prediction(df, features, label = \"coeff\"):\n",
    "    \"\"\"adds predicted coefficient, with KFold split\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): dataframe of data that was previously imported\n",
    "        features (list): parameter strings to include in model\n",
    "        label (str, optional): name of label property. Defaults to \"coeff\".\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "fit_labels = [\"c44_fcc\", \"intr_stack_fault_energy_fcc\", \"unstable_stack_energy_fcc\"]\n",
    "df_data = get_ip_prediction(df_data, fit_labels)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's look at our results.\n",
    "\n",
    "**Task**: write a function that takes in the dataframe, plots the actual coefficient on the horizontal axis, and plots the predicted coefficient on the vertical axis. Use seaborn scatterplot with \"species\" as the hue. Set the aspect ratio to \"equal\" for each axis, and plot a line along the diagonal to represent a perfect prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_vs_actual_plot(df, figsize = (5,5)):\n",
    "       \"\"\"plot predicted versus actual data using scatterplot\n",
    "\n",
    "       Args:\n",
    "           df (pandas.DataFrame): dataframe of data that was previously imported\n",
    "           figsize (tuple, optional): _description_. Defaults to (5,5).\n",
    "       \"\"\"\n",
    "\n",
    "\n",
    "       return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_vs_actual_plot(df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions from first-principles indicator properties\n",
    "\n",
    "Now, we will import the DFT data that we will use with our model to make inferred predictions based on the indicator properties. First, get the data.\n",
    "\n",
    "**Task**: Using pandas, download the CSV file with this [link](https://github.com/bjasperson/property-covariance-demo/blob/main/data/gb_dft.csv?raw=true), save the dataframe as `df_dft`, and look at the first five rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_link = \"https://github.com/bjasperson/property-covariance-demo/blob/main/data/gb_dft.csv?raw=true\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two sets of properties to consider now: one for the IP data, and one for the DFT data. We want to decide which model to use for the DFT data. Let's filter our k-fold CV results to only include the DFT canonical properties.\n",
    "\n",
    "**Task:** Write a function that filters your model performance list from above based on the DFT properties available. Take in the cv results dataframe and a list of DFT properties (from the df_dft columns). If the properties for a given row are all available with DFT results, include it ([hint](https://www.geeksforgeeks.org/python-check-if-the-list-contains-elements-of-another-list/)). Return a dataframe of new, filtered results. Take a look at the top 5 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_cv_results(df, dft_list):\n",
    "    \"\"\"takes in a df of k-fold CV results, and filters based on a list of properties\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): the k-fold CV df from above\n",
    "        dft_list (list): list of properties available from df_dft\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: filtered results from k-fold CV df\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    return \n",
    "\n",
    "df_result_dft_only = filter_cv_results(df_results, df_dft.columns.to_list())\n",
    "df_result_dft_only.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Make a list that identifies the factors from the top performing model. Create a function that calls your `linear_regr_pipeline()` to create a pipeline, fits the pipe to the IP data, and makes a prediction from DFT indicator properties. Add the results to `df_dft` under the name \"dft_pred_coeff\", and return `df_dft`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dft_prediction(model_features, df_dft):\n",
    "    \"\"\"get dft coefficient prediction and add to df as \"dft_pred_coeff\"\n",
    "\n",
    "    Args:\n",
    "        model_features (list): list of properties (str) to include in model\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: df_dft with prediction added \n",
    "    \"\"\" \n",
    "\n",
    "    # create new pipeline\n",
    "\n",
    "\n",
    "    # fit dft pipe to IP data\n",
    "\n",
    "\n",
    "    return df_dft\n",
    "\n",
    "# best model of the 5 available DFT properties\n",
    "dft_model_properties = ['c44_fcc',\n",
    "                        'intr_stack_fault_energy_fcc',\n",
    "                        'unstable_stack_energy_fcc',\n",
    "                    ]\n",
    "\n",
    "df_dft = get_dft_prediction(dft_model_properties, df_dft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the plotting function provided below to show a boxplot of coefficient results, along with the predicted versus actual DFT coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boxplot(df_ip, \n",
    "                df_dft, \n",
    "                plot_errorbar = True,\n",
    "                order_list = [\"Ag\",\"Al\",\"Au\",\"Cu\",\"Ni\",\"Pd\",\"Pt\"]):\n",
    "    \"\"\"plot boxplot of dft GB results\n",
    "    \"\"\"\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    fig, ax = plt.subplots(figsize=(4,3))\n",
    "    sns.boxplot(data = df_ip, \n",
    "                x=\"species\", \n",
    "                y=\"coeff\", \n",
    "                order=order_list, \n",
    "                color = \"0.8\", \n",
    "                linewidth=1.0,\n",
    "                fliersize=5.0,\n",
    "                whis=0,\n",
    "                flierprops={\"marker\":\".\"},\n",
    "                zorder=1)\n",
    "    ax.set_ylabel(\"GB scaling coefficient\")\n",
    "\n",
    "    # add dft Xs\n",
    "    ax.scatter(df_dft['species'],\n",
    "               df_dft['dft_pred_coeff'], \n",
    "               marker='x', \n",
    "               s=100., \n",
    "               alpha=1.0, \n",
    "               color=\"r\",\n",
    "               label='\\n'.join(wrap(r\"$E_0$ regression prediction using DFT indicator properties\",20)),\n",
    "               zorder=3)\n",
    "\n",
    "    df_dft_gb_plot = df_dft[['species','dft_exact_coeff']].drop_duplicates()\n",
    "    df_dft_gb_plot = df_dft_gb_plot[df_dft_gb_plot['species'].isin(order_list)]\n",
    "    ax.scatter(df_dft_gb_plot['species'],\n",
    "               df_dft_gb_plot['dft_exact_coeff'], \n",
    "               marker='<', \n",
    "               s=50., \n",
    "               alpha=0.9, \n",
    "               color=\"r\",\n",
    "               label='\\n'.join(wrap(r\"$E_0$ fit directly to DFT GB results\",20)),\n",
    "               zorder=2)\n",
    "\n",
    "    # add errorbars if desired\n",
    "    if plot_errorbar == True:\n",
    "        ax.errorbar(df_dft['species'],\n",
    "                    df_dft['regr_coeff'], \n",
    "                    yerr = (df_dft['regr_coeff_lower'],\n",
    "                            df_dft['regr_coeff_upper']), \n",
    "                            fmt='.', \n",
    "                            markersize=0.0001, \n",
    "                            alpha=0.5, \n",
    "                            color=\"r\",\n",
    "                            #label='\\n'.join(wrap(\"Predicted strength using DFT indicator properties\",20)),\n",
    "                            elinewidth=2.0,\n",
    "                            capsize = 4)\n",
    "\n",
    "    fig.legend(bbox_to_anchor = (0.05,0.9,0.85,.15),#(0.,1.02,1.,.102),\n",
    "                    loc='lower left',\n",
    "                    mode=\"expand\",\n",
    "                    ncol = 2,\n",
    "                    fontsize= 8)\n",
    "\n",
    "    return \n",
    "\n",
    "get_boxplot(df_data, df_dft, plot_errorbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We see that our model predictions match the DFT results pretty well. \n",
    "\n",
    "Additional areas to explore, as time permits:\n",
    "- Using different combinations of factors, try to find a better model!\n",
    "- Explore different model types other than linear regression: SVR, ensemble of regressors, ...\n",
    "    - see [here](https://scikit-learn.org/stable/machine_learning_map.html) for ideas\n",
    "- Repeat the process, this time with the strength data\n",
    "    - [URL link to data](https://github.com/bjasperson/property-covariance-demo/blob/main/data/strength_data.csv?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
