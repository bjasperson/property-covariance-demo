{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold, cross_val_score, LeaveOneOut, GridSearchCV\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model, svm\n",
    "\n",
    "from itertools import combinations\n",
    "from textwrap import wrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring covariance between properties across scales\n",
    "**Authors**: Benjamin Jasperson, Harley T. Johnson\n",
    "\n",
    "**GitHub URL**: https://github.com/bjasperson/property-covariance-demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background Information\n",
    "\n",
    "why\n",
    "compromise on modeling: accuracy/first-principles vs. speed and domain size.\n",
    "\n",
    "what\n",
    "\n",
    "goals\n",
    "\n",
    "We're interested in uncovering relationships between fundamental microscopic properties and GB energy.\n",
    "Desire a scalar value to compare against.\n",
    "Derived a scaling coefficient.\n",
    "For purposes here, you don't need to worry about how this coefficient is defined (see the paper for details)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data\n",
    "To begin, we need to import our project data.\n",
    "Collecting this data ....\n",
    "\n",
    "The data we will use is originally from [this manuscript](https://arxiv.org/abs/2411.16770), [published here](https://doi.org/10.1016/j.actamat.2025.120722). You can find the original paper repo [here](https://github.com/Johnson-Research-Group/gb_covariance).\n",
    "\n",
    "**Task**: Using pandas, download the CSV file with this [link](https://github.com/bjasperson/property-covariance-demo/blob/main/data/gb_data.csv?raw=true) and look at the first five rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_link = \"https://github.com/bjasperson/property-covariance-demo/blob/main/data/gb_data.csv?raw=true\"\n",
    "df_data = pd.read_csv(url_link, index_col=0)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data review\n",
    "It is important to get a feel for the data that you are working with. \n",
    "\n",
    "**Task**: import the label csv file, located at `./data/label_dict.csv`. Convert it to a dictionary for use with plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_label_dict(label_dict_path = \"./data/label_dict.csv\"):\n",
    "    \"\"\"import the ./data/label_dict.csv file, convert it to a dictionary\n",
    "\n",
    "    Args:\n",
    "        label_dict_path (str, optional): path to csv file with key/value pairs. Defaults to \"./data/label_dict.csv\".\n",
    "\n",
    "    Returns:\n",
    "        dict: a dictionary with df column names as keys, and human-readable label as value\n",
    "    \"\"\"\n",
    "    \n",
    "    # useful for plotting purposes\n",
    "    df_label_dict = pd.read_csv(label_dict_path)\n",
    "    label_dict = df_label_dict.to_dict(orient=\"records\")[0]\n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to look at how the different fundamental properties relate with each other and our Quantity of Interest (GB energy coefficient). To do this, we need to create a plotting function.\n",
    "\n",
    "**Task**: create a function to plot a set of parameters from the dataframe using Seaborn pairplot. Explore different combinations of pairplots for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairplot_fun(df, \n",
    "                 params_list,  \n",
    "                 label_dict, \n",
    "                 height=1.5,\n",
    "                 #xlims = False, \n",
    "                 ):\n",
    "    \"\"\"create pairplot for select indicator properties\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): dataframe of data that was previously imported\n",
    "        params_list (list): list of parameter strings to use\n",
    "        label_dict (dict): the label dict from above, to convert label keys to human readable format\n",
    "        height (float): pairplot height. Defaults to 2.0.\n",
    "        xlims (list): list of limits to use for plotting\n",
    "    \"\"\"\n",
    "    params_list.extend(['species'])\n",
    "    X = df[params_list]\n",
    "    X.columns = [label_dict[x] for x in X.columns.to_list()]\n",
    "    sns.set_theme(style=\"white\", font_scale = 1)\n",
    "    marker_list = ['o','^','v','<','>','s','D','p','X','*','.','P']\n",
    "    g = sns.pairplot(X, hue='species', corner=True, markers = marker_list[0:len(df.species.drop_duplicates())],\n",
    "                     plot_kws=dict(s=50, linewidth=0.1, rasterized = True), \n",
    "                     height=height,\n",
    "                     diag_kind='hist')\n",
    "    sns.move_legend(g, \"upper right\", bbox_to_anchor = (0.85,1))\n",
    "\n",
    "    # if xlims != False:\n",
    "    #     for i in range(len(g.axes)):\n",
    "    #         for j in range(len(g.axes[i])):\n",
    "    #             g.axes[i][j].set_xlim(xlims)\n",
    "    #             g.axes[i][j].set_ylim(xlims)\n",
    "\n",
    "\n",
    "label_dict = import_label_dict()\n",
    "plot_param_list = [\"lattice_constant_fcc\", \"lattice_constant_bcc\", 'intr_stack_fault_energy_fcc', 'unstable_stack_energy_fcc']\n",
    "pairplot_fun(df_data, plot_param_list, label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying indicator properties\n",
    "\n",
    "**Task**: run K-fold cross-validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop regression model\n",
    "Linear regression versus SVR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Write a function that returns a linear regression pipeline. It should include:\n",
    "- StandardScaler: for scaling the input values; helpful for SVR, used in LR for consistency. \n",
    "- KNNImputer: for filling in the missing property values in our data. Set `n_neighbors=2` and `keep_empty_features=True`\n",
    "- LinearRegression: the linear regression model.\n",
    "Combine these in a `Pipeline` to facilitate easy use ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regr_pipeline():\n",
    "    \"\"\"creates a linear regression pipeline for modeling purposes\n",
    "\n",
    "    Returns:\n",
    "        sklearn.pipeline.Pipeline: linear regression pipeline\n",
    "    \"\"\"\n",
    "\n",
    "    # Linear regression selection pipeline, used in manuscript\n",
    "    imput = KNNImputer(n_neighbors=2,\n",
    "                       keep_empty_features=True)\n",
    "    model = linear_model.LinearRegression()\n",
    "\n",
    "    pipe = Pipeline(steps=[('scale', StandardScaler()),\n",
    "                            ('imp', imput),\n",
    "                            ('lr', model)])\n",
    "    return pipe\n",
    "\n",
    "pipe = linear_regr_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to make a list of the properties we are interested in considering as indicator properties for our model. A starting point is provided below.\n",
    "\n",
    "**If time**: explore different properties, and see if any new additions result in improved models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of parameters to explore\n",
    "params_list = ['lattice_constant_fcc',\n",
    "                'bulk_modulus_fcc', 'c11_fcc', 'c12_fcc', 'c44_fcc',\n",
    "                'extr_stack_fault_energy_fcc', 'intr_stack_fault_energy_fcc', 'unstable_stack_energy_fcc'\n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the parameter list, we want to make a list of possible combinations up to a certain number of factors. \n",
    "\n",
    "**Task**: write a function that uses the `combinations` package to make a list of lists, with possible combinations of factors up to `n_factor_max`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subsets_list(factor_list, n_factor_max):\n",
    "    subsets = []\n",
    "    for n in range(1, (n_factor_max+1)):\n",
    "        for subset in combinations(factor_list, n):\n",
    "            subsets.append(list(subset))\n",
    "    return subsets\n",
    "\n",
    "subsets = create_subsets_list(params_list, n_factor_max = 2)\n",
    "print(subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our list of possible factor combinations to consider. If we fit our model to the entire dataset, we aren't getting a realistic picture of the test error. So, we will use the `cross_val_score` package from scikit-learn to perform cross validation. Read more about it [here](https://scikit-learn.org/stable/modules/cross_validation.html).\n",
    "\n",
    "**Task**: write a function that iterates over each subset of the possible factor combinations. For each subset, use the `cross_val_score` package to perform cross-validation and return a score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factor_select_cv(df, factors, pipe, subsets, label=\"coeff\", cv=5, scoring='r2'):\n",
    "    \"\"\"evaluates combinations of factors\n",
    "    \n",
    "    return a dataframe that includes one row per factor combination. list of parameters w/ cv score\n",
    "    \"\"\"\n",
    "    X = df[factors]\n",
    "    y = df[label]\n",
    "    \n",
    "    cv_score_mean = []\n",
    "    cv_score_std = []\n",
    "    num_factors = []\n",
    "    for i, subset in enumerate(subsets):\n",
    "        print(f\"{i} of {len(subsets)}\")\n",
    "        print('current subset: ', subset)\n",
    "        score = cross_val_score(pipe, X[subset], y, cv=cv, scoring=scoring, n_jobs = -1)\n",
    "        print('score mean: ', np.mean(score))\n",
    "        cv_score_mean.append(np.mean(score))\n",
    "        cv_score_std.append(np.std(score))\n",
    "        num_factors.append(len(subset))\n",
    "\n",
    "    df_results = pd.DataFrame({'factors': subsets,\n",
    "                               'num_factors': num_factors,\n",
    "                               'cv_score': cv_score_mean,\n",
    "                               'cv_score_std': cv_score_std})\n",
    "    df_results = df_results.sort_values('cv_score', ascending=False)\n",
    "\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = TransformedTargetRegressor(regressor=pipe,\n",
    "#                                   transformer=StandardScaler())\n",
    "filename = \"kfold_models_lr\"\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=5)\n",
    "df_results = factor_select_cv(df_data,\n",
    "                              params_list,\n",
    "                              pipe, \n",
    "                              subsets, \n",
    "                              cv=cv, \n",
    "                              scoring='neg_root_mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If time**: repeat the steps above using different modeling approaches, e.g., support vector regression. Compare performance of the models, and try to understand why some perform better than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.sort_values(\"cv_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick the top performing model and make a regression model to use. Make predictions and compare predicted versus actual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pred(df_in, pipe_in, factors, label=\"coeff\"):\n",
    "    X = df_in[factors]\n",
    "    y = df_in[label]\n",
    "    pipe_in.fit(X, y)\n",
    "    y_pred = pipe_in.predict(X)\n",
    "    return y_pred\n",
    "\n",
    "df_data[\"coeff_pred\"] = add_pred(df_data, pipe, [\"c12_fcc\", \"c44_fcc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: write a function that takes in the dataframe, plots the actual coefficient on the horizontal axis, and plots the predicted coefficient on the vertical axis. Use seaborn scatterplot with \"species\" as the hue. Set the aspect ratio to \"equal\" for each axis, and plot a line along the diagonal to represent a perfect prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_vs_actual_plot(df, \n",
    "                        figsize = (5,5),\n",
    "                        ):\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig,ax = plt.subplots(figsize = (figsize[0],figsize[1]))\n",
    "    sns.scatterplot(data=df, x='coeff', y='coeff_pred',hue='species',style = 'species')\n",
    "    ax.plot(np.linspace(0,max(df['coeff']),50),\n",
    "           np.linspace(0,max(df['coeff']),50))\n",
    "    ax.set_xlabel(r\"actual coefficient [$J/m^2$]\", fontsize=8)\n",
    "    ax.set_ylabel(r\"predicted coefficient [$J/m^2$]\", fontsize=8)\n",
    "    ax.tick_params(labelsize=8)      \n",
    "    ax.axes.set_aspect('equal')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_vs_actual_plot(df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions from first-principles indicator properties\n",
    "\n",
    "Now, we will import the DFT data that we will use with our model to make inferred predictions based on the indicator properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_link = \"https://github.com/bjasperson/property-covariance-demo/blob/main/data/gb_dft.csv?raw=true\"\n",
    "df_dft = pd.read_csv(url_link, index_col=0)\n",
    "df_dft.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter your model performance list from above based on the DFT properties available, and select the top performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_model_results(df, dft_list):\n",
    "    results = []\n",
    "    for i, row in df.iterrows():\n",
    "        factors = row.factors\n",
    "        if all(factor in dft_list for factor in factors):\n",
    "            results.append(row)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "df_result_dft_only = filter_model_results(df_results, df_dft.columns.to_list())\n",
    "df_result_dft_only.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make prediction from DFT indicator properties, compare with the \"true\" DFT coefficient (calculated separately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model of the 5 available DFT properties\n",
    "dft_model_properties = ['c44_fcc',  \n",
    "                        'unstable_stack_energy_fcc'\n",
    "                       ]\n",
    "\n",
    "df_dft['dft_pred_coeff'] = add_pred(df_dft, \n",
    "                                    pipe,\n",
    "                                    dft_model_properties,\n",
    "                                    label=\"dft_exact_coeff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below is from dft_import. Need to figure out what to roll in. Without prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boxplot(df_ip, \n",
    "                df_dft, \n",
    "                plot_errorbar = True,\n",
    "                order_list = [\"Ag\",\"Al\",\"Au\",\"Cu\",\"Ni\",\"Pd\",\"Pt\"]):\n",
    "    \"\"\"plot boxplot of dft GB results\n",
    "    \"\"\"\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    fig, ax = plt.subplots(figsize=(4,3))\n",
    "    sns.boxplot(data = df_ip, \n",
    "                x=\"species\", \n",
    "                y=\"coeff\", \n",
    "                order=order_list, \n",
    "                color = \"0.8\", \n",
    "                linewidth=1.0,\n",
    "                fliersize=5.0,\n",
    "                whis=0,\n",
    "                flierprops={\"marker\":\".\"},\n",
    "                zorder=1)\n",
    "    ax.set_ylabel(\"GB scaling coefficient\")\n",
    "\n",
    "    # add dft Xs\n",
    "    ax.scatter(df_dft['species'],\n",
    "               df_dft['dft_pred_coeff'], \n",
    "               marker='x', \n",
    "               s=100., \n",
    "               alpha=1.0, \n",
    "               color=\"r\",\n",
    "               label='\\n'.join(wrap(r\"$E_0$ regression prediction using DFT indicator properties\",20)),\n",
    "               zorder=3)\n",
    "\n",
    "    df_dft_gb_plot = df_dft[['species','dft_exact_coeff']].drop_duplicates()\n",
    "    df_dft_gb_plot = df_dft_gb_plot[df_dft_gb_plot['species'].isin(order_list)]\n",
    "    ax.scatter(df_dft_gb_plot['species'],\n",
    "               df_dft_gb_plot['dft_exact_coeff'], \n",
    "               marker='<', \n",
    "               s=50., \n",
    "               alpha=0.9, \n",
    "               color=\"r\",\n",
    "               label='\\n'.join(wrap(r\"$E_0$ fit directly to DFT GB results\",20)),\n",
    "               zorder=2)\n",
    "\n",
    "    # add errorbars if desired\n",
    "    if plot_errorbar == True:\n",
    "        ax.errorbar(df_dft['species'],\n",
    "                    df_dft['regr_coeff'], \n",
    "                    yerr = (df_dft['regr_coeff_lower'],\n",
    "                            df_dft['regr_coeff_upper']), \n",
    "                            fmt='.', \n",
    "                            markersize=0.0001, \n",
    "                            alpha=0.5, \n",
    "                            color=\"r\",\n",
    "                            #label='\\n'.join(wrap(\"Predicted strength using DFT indicator properties\",20)),\n",
    "                            elinewidth=2.0,\n",
    "                            capsize = 4)\n",
    "\n",
    "    fig.legend(bbox_to_anchor = (0.05,0.9,0.85,.15),#(0.,1.02,1.,.102),\n",
    "                    loc='lower left',\n",
    "                    mode=\"expand\",\n",
    "                    ncol = 2,\n",
    "                    fontsize= 8)\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_boxplot(df_data, df_dft, plot_errorbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
